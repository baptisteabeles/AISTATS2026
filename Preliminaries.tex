

%\section{Preliminaries}

%Before being able to formally define our setting and present our algorithms, we first provide more background on linear 
%stochastic bandits, as well as an overview of the most common algorithmic ideas used to address such problems. The 
%contents in this section are standard and can be found in the excellent book \citet{lattimore2020bandit}.
%\label{sec::preliminaries}
%\eug{We need to make it clear that this is all well known stuff and that we are making summary of classical case. And 
%that this is useful to introduce ideas and notation and that later we will provide our results in the section that will 
%follow...}

\section{Preliminaries on linear bandits}
% \textit{In what follows, we will refer to the learner of the bandit problem as the agent and use the term 
% \textit{player} for other sequential game.}
\label{subsec:LSB}
We consider a version of the classic problem of regret minimisation in stochastic linear bandits, where an agent needs to 
make a sequence of decisions (or pick an \emph{arm}) from a given contextual decision set that may change over the 
sequence of rounds. We assume that the environment is oblivious to the actions of the 
agent, in the sense that the decision sets are determined in advance, and do not depend neither on the realisations of 
the noise nor on the agent's arm-selection strategy. 

Concretely, we define the problem as follows. Let $\theta^\star\in\real^p$ be a parameter vector that is unknown to the 
learning agent. We assume as known an upper bound $B>0$ on its euclidean norm (namely, $\theta^\star\in\mathcal B(B)$). Fix 
a sequence of decision sets $(\X_t)_{t\geq 1}$ in $\real^p$. We assume that for all $t$ we have $\X_t\subseteq\mathcal B(1)$.  At each round $t$, the agent is required to pick an arm 
$X_t\in\X_t$, and receives the reward $Y_t = \siprod{\theta^\star}{X_t} + \ee_t$. The sequence $(\ee_t)_{t\geq1}$ 
represents the random feedback noise. The noise across different rounds is typically assumed to be conditionally centred 
and to have well behaved tails. For instance, a common assumption is to ask that $\E[\ee_t|\F_{t-1}]$ %\eug{later $\F_t$ is used as something that contains $X_{t+1}$ as well. This is very confusing and needs a fix. Either we use different notation or we use the same filtration...}
is centred and 
sub-Gaussian, where $\F_{t} = \sigma(\ee_1,\dots, \ee_t)$ is the $\sigma$-field generated by the noise. This is the assumption this work relaxes. We also remark that, more generally, one can consider the case where the $X_t$ as well are randomised, namely contain additional randomness that is not included in the noise. To take this into account, one can add this other source or randomness in the filtration. However, since in our case we will only consider a non-randomised bandit algorithm, we omit this to simplify our analysis. 


The agent aims to find a good strategy to pick arms $X_t$ that lead to a high expected $T$-round reward 
$\sum_{t=1}^T\siprod{X_t}{\theta^\star}$. To compare their performance to that of an agent playing each round the best 
available arm (in expectation), we define the \emph{regret} after $T$ rounds as
\[\Reg(T) =\sum_{t=1}^t \sup_{x\in\X_t}\big( \siprod{x}{\theta^\star} - \siprod{X_t}{\theta^\star}\big)\,.\]

A common approach to tackle the linear bandit problem is to follow an \emph{upper confidence bound} (UCB) strategy. This 
involves the following protocol. At each round $t$, we first derive a confidence set $\C_{t-1}$, based on the 
arm-reward pairs $(X_s, Y_s)_{s\leq t-1}$. This is a random set (as it depends on the past noise realisations), which 
must be constructed ensuring that $\theta^\star\in \C_{t-1}$ with high probability. More precisely, the regret can be 
effectively controlled if one  can ensure that $\theta^\star$ uniformly belongs to every set $(\C_t)_{t\geq 1}$, with 
high probability (a property often referred to as \emph{anytime validity}). Then, for every available arm $x$, 
we let $$\ucb_{\C_{t-1}}(x) = \max_{\theta\in \C_{t-1}}\,\siprod x\theta\,.$$ By definition, this is a high-probability 
upper bound on $\siprod x{\theta^\star}$, which justifies the name ``upper confidence bound''. The idea is then to 
\emph{optimistically} pick as $X_t\in\X_t$ the arm maximising $\ucb_{\C_{t-1}}$. 


%We will refer to this meta algorithm (Algorithm \ref{alg::LinUCB}) as LinUCB. The main challenge for this approach typically lies in the construction of the confidence sets. Specifically, to obtain valid guarantees on the regret of this strategy one typically needs to construct a sequence of confidence sets that hold uniformly in high probability for all steps (\emph{anytime} validity). Such sequences are commonly referred to as confidence sequences. 

%\begin{algorithm}[!h]
%\caption{LinUCB}\label{alg::LinUCB}
%\begin{algorithmic}
%(\frac{1}{K},...,\frac{1}{K})
%\For {$t =2,\dots ,n$}
%\State Construct $C_{t-1}$ from $\{(x_s,y_s)\}_{s=1}^{t-1}$;
%\State Observe the next action set $\X_{t}$;
%\State Play $x_{t} = \arg \max_{x \in \X}\%{\ucb_{C_{t-1}}(x)\}$;
%\State Receive the reward reward $y_{t} = %\langle x_t, \theta^\star\rangle + \ee_t$.
%\EndFor
%\end{algorithmic}
%\end{algorithm} 

A key technical challenge in designing a UCB algorithm is to construct the anytime valid confidence sequence $(\C_t)_{t\geq 1}$. Typically, under sub-Gaussian assumptions on the noise, these sets take the form of an ellipsoid, centred on a (regularised) maximum likelihood estimator. Explicitly, we often have
$$\C_t = \big\{\theta\in\Theta\,:\,\|\theta - \widehat{\theta}_{t}\|_{V_{t}}^2 \leq \beta_t^2\big\}\,,$$
where $\wh{\theta}_t$ is the least-squares estimator of $\theta^\star$, $V_{t}$ is the \emph{feature-covariance} matrix 
and $\beta_t$ is a radius carefully chosen so that the high-probability coverage 
requirement is satisfied. In this work, to construct the csonfidence sets we will leverage an 
\emph{online-to-confidence-set-conversion} approach, a method that reduces the problem of proving statistical 
concentration bounds to proving existence of well-performing algorithms for an associated game of \emph{sequential 
probability assignment}. We refer to \Cref{sec:conf_sets} for more details on our technique to construct the confidence 
sequence. 

%\grg{Brief description of the standard confidence ellipsoid comes here. If it ends up being reasonably short, then the 
%two subsections can be merged into one.}

% \subsection{Online-to-confidence set conversion}
% \label{subsec::conf_set}
% We now describe a general method to construct confidence sequences for the standard linear bandit problem, via a reduction to regret analysis of online learning algorithms. This approach, named \emph{online-to-confidence set} conversion, was first proposed by \cite{abbasi2011improved}, and has later been extended and improved by several works (\cite{clerico2025confidencesequencesgeneralizedlinear} \eug{add more stuff... there are many refs...}).
% 
% In what follows, we fix a confidence level $\delta\in(0,1)$. A confidence sequence $(C_t)_{t\geq 1}$ for $\theta^\star$ is a sequence of random sets such that
% \[\mathbb{P}\big( \theta^\star \in C_t,\,\forall t\geq 1\big) \geq 1-\delta\,,\]
% where the randomness of the $C_t$ is due to the noise sequence $(\ee_t)_{t\geq 1}$. 
% We will focus on confidence sets and sequences that can be defined as the set of parameters $\theta$ whose likelihood $p(y_t|x_1,\dots,x_t,\theta)$ is nearly maximal, namely $C_t$ takes the form \eug{Notation $p$ has never be defined... what if $\ee_t$ does not admit a density?}
% \[C_t= \left\{ \theta \in \Theta\,:\, \tfrac{\prod_{s=1}^t p(y_s|x_s,\theta)}{\sup_{\theta' \in \Theta\prod_{s=1}^t p(y_s|x_s,\theta')}} \geq e^{-\beta_t}\right\}\,, \]
% for some suitably defined $\beta_t$. 
% In the standard linear stochastic problem that we have been discussing so far, it is well known that these confidence sets can be equivalently \eug{I don't like this \emph{equivalently}... If they were Gaussian it would be equivalent, but for sub-Gaussian it is a relaxation to use the Gaussian likelihood... Since we are only considering the sub-Gaussian case shan't we just put as form for $C_t$ the one with the quadratic loss?} written in terms of the squared loss defined as  $\ell_t(\theta) = \frac{1}{2}(\siprod{x_t}{\theta}-y_t)^2$. This gives raise to an ellipsoid centred around the \emph{maximum likelihood estimator} (MLE) at round $t$, $\wh\theta_t = \arg \min_{\theta \in \Theta} \LL_{t}(\theta)$, in the form  
% \begin{equation}
% \label{eq:ellipsoid}
%     C_t = \big\{\theta \in \Theta, \LL_t(\theta)-\LL_t(\wh \theta_t) \leq \beta_t\big\}\,.
% \end{equation}
% 
% 
% Here, $\LL_t = \sum_{s=1}^t \ell_s$ denotes the cumulative loss over $t$ rounds, and  The coefficients $\beta_t$ dictate the width of the ellipsoids and need to be chosen in a way that ensure sequential coverage. In order to obtain a suitable choice, we will use an abstract online learning game which we present below. \eug{To be rewritten...}
% %We will denotecorresponding to the loss function associated to the statistical model and naturally extends its definition 
% %with respect to the space of measure over $\Theta$ by denoting for $Q \in \Delta_{\Theta},$ $\ell_t(Q) := -\log(\int\exp(-\ell_t(\theta))\mathrm{d}Q(\theta))$.
% 
% Consider the following game between an online player and their environment. At each round $s= 1, 2, \dots, t$:
% \begin{itemize}
% 
% \item The environment reveals $x_s$ to the player;
% \item The player picks $\theta_s\in\Theta$ (or more generally a distribution $p_s \in \Delta_\Theta$);
% \item The environment reveals $y_s$ to the player;
% \item The player incurs the loss $\ell_s(\theta_s)$.
% \end{itemize}
% Let us denote as $\Pi=(\theta_s)_{s=1\dots t}$ the sequence of parameters chosen by the online player. We define their regret with respect to a fixed comparator $\theta$ as
% \[ \Regret_{\Pi,t}(\theta) = \sum_{s=1}^t \big(\ell_s( \theta)-\ell_s(\theta_s)\big)\,.\]
% 
% By adding and subtracting the cumulative loss of the player, we can rewrite the excess log-likelihood of the true parameter $\theta^\star$,
% relative to the MLE $\wh\theta_t$, as
% \begin{equation}
% \label{eq:regret+martingale}
%     \LL_t(\theta^\star)-\LL_t(\wh \theta_t)  
%     = \underbrace{\sum_{s=1}^t \big(\ell_s(\theta_s)-\ell_s(\wh \theta_t)\big) }_{\text{regret term : }\Regret_{\Pi,t}(\wh \theta_t)}  + \underbrace{\sum_{s=1}^t \big(\ell_s(\theta^\star)-\ell_s(\theta_t)\big)}_{M_t(\theta^\star)\leq_{\delta} \log \frac{1}{\delta}}\,.
% \end{equation}
% 
% 
% The above decomposition is the first step to build our confidence sets. Note that the only assumption we make about the parameters chosen by the online player is that they are \textit{predictable}, which formally means that for any $s>1, \theta_s$ is $\F_{s-1}$-measurable where $\F_{s-1}:=\sigma(x_1,y_1,\dots,x_{s-1},y_{s-1},x_s)$. The regret term is a well-studied object in the online learning literature, with known tight/minimax upper bounds. On the other hand, one can easily notice that $M_t(\theta^\star)$ is the log of a non-negative super-martingale (see Appendix \ref{app:NNSMG} for a detailed proof), and hence is small with high probability. More precisely, from Ville's inequality we have
% \[\mathbb{P}\left( \exists t>0\,:\,M_t(\theta^\star) \geq \log\tfrac{1}{\delta}\right) \leq \delta\,.\]
% 
% Hence, defining for all $t\geq 1$
%  \begin{equation}
% \label{eq:ConfidenceSequence}
%     C_t(\delta)= \left\{\theta\,:\, \LL_{t}(\theta)-\LL_t(\wh \theta_{t}) \leq \Regret_{\Pi,t}(\wh \theta_{t})+\log \frac{1}{\delta}\right\}\,,
% \end{equation} yields an anytime valid confidence sequence. Also note that in the above form $C_t(\delta)$ is an ellipsoid  centered at $\wh \theta_{t}$ with width $\beta_t(\delta)= \Regret_{\Pi,t}(\wh \theta_{t})+\log\frac{1}{\delta}.$
% 
% 
% The best known bounds on this regret term are of order $\mathcal{O}(\max_{s \in [t]}y_s^2 p\log(t))$. The maximum squared label can be bounded by a constant times $\log(t)$, so we get $\mathcal{O}(p\log(t)^2)$. However, we would like the RHS to be of order $\mathcal{O}(p\log(t))$. We can remove the dependence on the max label by switching to randomised online learning algorithms, which return a sequence of distributions $Q_1, Q_2, \dots, $ on the set of parameter vectors. We naturally extend the definition of $\LL$ to $\Delta_\Theta$ where $\Delta_\Theta$ denote the set of probability distributions over $\Theta$ by denoting :
% $$\mathcal{L}_s(Q) := -\log(\int\exp(-\ell_s(\theta))\mathrm{d}Q(\theta)).$$
% We can then write
% \begin{equation}
% \sum_{s=1}^t\ell_s(\theta^\star)-\ell_s(\wh \theta_t) = \underbrace{\sum_{s=1}^t \mathcal{L}_s(Q_s)-\ell_s(\wh \theta_t) }_{\text{regret term}}  + \underbrace{\sum_{s=1}^t \ell_s(\theta^\star)-\mathcal{L}_s(Q_s)}_{\text{log non-negative martingale term}}\label{eqn:randomised_reg_plus_mart}
% \end{equation}
% 
%     As long as the distributions $(Q_s)_{s>1}$ are predictable w.r.t. $(\mathcal{F}_s)_{s>1}$, then the log martingale term is still the log of a non-negative supermartingale. If we let $M_t = \exp(\sum_{s=1}^{t}\ell_t(\theta^\star) - \mathcal{L}_s(Q_s))$, then by using the fact that each $Q_s$ is $\mathcal{F}_{s-1}$-measurable,
% \begin{align*}
% \mathbb{E}[M_n|\mathcal{F}_{t-1}] &= M_{t-1}\mathbb{E}[\exp(\ell_t(\theta^\star) - \mathcal{L}_t(Q_t))|\mathcal{F}_{t-1}]\\
% &= M_{t-1}\mathbb{E}\left[\int\exp(\ell_t(\theta^\star) - \ell_t(\theta))\mathrm{d}Q_t(\theta)\Big|\mathcal{F}_{t-1}\right]\\
% &= M_{t-1}\int\mathbb{E}[\exp(\ell_t(\theta^\star) - \ell_t(\theta))|\mathcal{F}_{t-1}]\mathrm{d}Q_n(\theta)\\
% &\leq M_{t-1}\,,
% \end{align*}
% 
% where the last inequality follows from the proof of Lemma \ref{lemma:Concentration_SMG_classic} (which shows that $\mathbb{E}[\exp(\ell_t(\theta^\star) - \ell_t(\theta))|\mathcal{F}_{t-1}] \leq 1$). \eug{Since it's in the appendix anyway I think we should just write it explicitly as a corollary...} The regret term is now the regret of $(Q_s)_{s \in [t]}$ in a problem called \emph{sequential probability assignment}. There are algorithms for sequential probability assignment that have regret of order $\mathcal{O}(p\log(t/p))$, which gives us a RHS with the right dependence on $p$ and $n$. In this setting, if we use the Exponentially Weighted Average (EWA) forecaster to generate $Q_1, Q_2, \dots$, then we can write down a closed-form expression for this regret term. The EWA forecaster (with learning rate 1) is defined by
% \begin{equation*}
% \frac{\mathrm{d}Q_t}{\mathrm{d}Q_1}(\theta) = \frac{\exp(-\sum_{s=1}^{t-1}\ell_s(\theta))}{\int \exp(-\sum_{s=1}^{t-1}\ell_s(\theta))\mathrm{d}Q_1(\theta)}\,.
% \end{equation*}
% 
% Since $\ell_t$ is the negative log likelihood (if the noise was actually Gaussian), this is equivalent to Bayesian linear regression. Using the definition of $Q_s$, we can write the total log loss as
% \begin{align*}
% \sum_{s=1}^{t}\mathcal{L}_s(Q_s) &= \sum_{s=1}^{t}-\log\int\exp(-\ell_s(\theta))\mathrm{d}Q_s(\theta)\\
% &= \sum_{s=1}^{t}-\log\left(\frac{\int \exp(-\sum_{s=1}^{t}\ell_s(\theta))\mathrm{d}Q_1(\theta)}{\int \exp(-\sum_{s=1}^{t-1}\ell_s(\theta))\mathrm{d}Q_1(\theta)}\right)\\
% &= -\log\int \exp\left(-\sum_{s=1}^{t}\ell_t(\theta)\right)\mathrm{d}Q_1(\theta)\,.
% \end{align*}
% 
% By subtracting $\sum_{s=1}^{t}\ell_s(\wh\theta_t)$ from both sides, we see that the regret term is
% \begin{equation}
% \sum_{s=1}^{t}\mathcal{L}_s(Q_s) - \sum_{s=1}^{t}\ell(\wh\theta_t) = -\log\int \exp\left(-\sum_{s=1}^{t}\ell_s(\theta) + \sum_{s=1}^{t}\ell_s(\wh\theta_t)\right)\mathrm{d}Q_1(\theta)\,.\label{eqn:ewa_regret}
% \end{equation}
% 
% If $Q_1$ is Gaussian, this integral can be evaluated using the Gaussian integral formula (see next section).
    
